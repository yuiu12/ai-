信息论是运用概率论与数理统计的方法研究信息、信息熵、通信系统、数据传输、密码学、数据压缩等问题的应用数学学科。信息论中包含的知识和概念在机器学习中也有应用，典型的例子是其核心思想『熵』的应用。
熵
    1854年由克劳休斯提出的一个用来度量体系混乱程度的单位，阐述了热力学第二定律熵增原理
    在孤立系统中，体系与环境没有能量交换，体系总是自发的向混乱度增大的方向变化，整个系统的熵值越来越大
    熵越大，表征的随机变量的不确定度越大，含有的信息量越多

联合熵
    度量一个联合分布的随机系数的不确定度。
    物理意义，是观察一个多随机变量的随机系统获得的信息量，是对二维随机变量(X,Y)不确定性的度量。

条件熵
    [在随机变量X发生的前提下，随机变量Y发生新带来的熵]
    物理意义  得知某一确定信息的基础上获取另外一个信息时所获得的信息量，衡量在已知随机变量的X条件下，随机变量Y的不确定性

相对熵
    在信息论中用来描述两个概率分布差异的熵，KL散度。
    相对熵表示当用概率分布Q来拟合真实分布P时，产生的信息损耗

交叉熵
    信息论中用来度量两个概率分布间的差异性。
    使用机器学习训练网络时，输入数据与标签通常是确定的。那么，真实概率分布P(x)是确定的，熵H(P(x))是一个可以确定的常量
    交叉熵=相对熵 + 熵 = 相对熵 + 一个常量。 交叉熵也可以用来描述真实概率分布P(x)与预测概率分布Q(x)的差异，且交叉熵的计算公式更简单。
    交叉熵损失函数来计算Loss

互信息
    信息论里一种有用的信息度量方式，看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。

常用等式
1.条件熵、联合熵与熵之间的关系
2.条件熵、联合熵与互信息之间的关系
3.互信息的定义
    I(X,Y) = H(X) + H(Y) - H(X,Y) 

最大熵模型
    机器学习领域，概率模型学习过程中有一个最大熵原理，学习概率模型时，所有可能的概率分布中，熵最大的模型是最好的模型
    通常用约束条件来确定模型的集合，最大熵模型原理表诉为；在满足越苏条件的模型集合中，选取熵最大的模型
    直观地看，最大熵原理认为：
        要选择概率模型，首先必须满足已有的事实，约束条件
        在没有更多信息的情况下，那些不确定的部分都是【等可能的】。最大熵原理通过熵的最大化表示等可能性；【等可能】不易操作；而熵则是一个可优化的指标
        
